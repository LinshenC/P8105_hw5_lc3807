---
title: "p8105_hw5_lc3807"
author: "Linshen Cai"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load necessary package
library(tidyverse)
set.seed(1)
```
## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides.

## Problem 2

```{r}
data = tibble(files = list.files("data/"),path = str_c("data/", files)) |> 
  mutate(data = map(path, read_csv)) |> 
  unnest() |> 
  mutate(
    group = str_sub(files, 1, 3),
    ID = str_sub(files, 5, 6)) |> 
  pivot_longer(cols = starts_with("week"), names_to = "week", values_to = "value")|>
  mutate(week = as.numeric(substring(week, 6,6))) |> 
  mutate(value = as.numeric(value)) |> 
  dplyr::select(group, ID, week, value)

data|>
  group_by(group, ID)|>
  ggplot(aes(x = week, y = value, group = interaction(group, ID), color = group))+
  geom_line()+
  facet_grid(~group)+
  labs(x = "Week", y = "Value", title = "observations on each subject over time")

```

The observations for each patient in the control group fluctuate around a consistent value every few weeks. On the other hand, each subject's observations in the experiment group show tendencies of increasing throughout the course of the weeks.


## Problem 3

```{r}

n = 30
sigma = 5
alpha = 0.05

ttest = function(n,mean,sigma){
  sim_data = tibble(
    x = rnorm(n,mean = mean, sd = sigma)
  )
  
  tstats = broom::tidy(t.test(sim_data$x, mean = mean, conf.level = 0.95, mu = 0))
  mu_hat=tstats$estimate[1]
  p_value = tstats$p.value[1]
  
  results = tibble(mu_hat,p_value)
  results
}

generate_5000 = function(mean){
  output = list()
  for(i in 1:5000){
  output[[i]] = tibble(
    ttest(n,mean,sigma))
  }
sim_results = bind_rows(output) |> 
  mutate(mean = mean)
}
mu_0=generate_5000(0)
```
The mu_0 table contain all mu hats and p values of t test based on random 5000 datasets when we set mean of population is 0.

```{r}
mu_1=generate_5000(1)
mu_2=generate_5000(2)
mu_3=generate_5000(3)
mu_4=generate_5000(4)
mu_5=generate_5000(5)
mu_6=generate_5000(6)
```

```{r}
mu_all = bind_rows(mu_0,mu_1,mu_2,mu_3,mu_4,mu_5,mu_6)

power = mu_all |> 
  group_by(mean) %>% 
  summarize(prop_rejected = sum(p_value < 0.05)/5000) 

plot_power = ggplot(power,aes(x = mean,y = prop_rejected)) +
  geom_point() +
  geom_line() +
  labs(title = "Power vs. True Effect Size",
       x = "True μ",
       y = "Power") +
  theme_minimal()

plot_power
```

We may see that the rejection rate increases as true_mu increases. Thus, as the impact size increases, so does the power.

```{r}
average_estimate_mean = mu_all |> 
  group_by(mean) |> 
  summarize(average_estimate = mean(mu_hat))

plot_compare = ggplot(average_estimate_mean,aes(x = mean,y = average_estimate)) +
  geom_point() +
  geom_line() +
  labs(x = "True μ",y = "Average estimate mean",title = "Average estimate mean vs. true μ") +
  theme_minimal()

plot_compare
```

The average estimate mean is rather near to the genuine mean, as can be seen from the above. The genuine mean and average estimate mean have a connection that is about y = x.

```{r}
average_estimate_mean_rejected = mu_all |> 
  filter(p_value < 0.05) |> 
  group_by(mean) |> 
  summarize(average_estimate = mean(mu_hat))

ggplot() +
  geom_line(data = average_estimate_mean, aes(x = mean, y = average_estimate),color = "blue") +
  geom_point(data = average_estimate_mean, aes(x = mean, y = average_estimate),color = "blue") +
  geom_line(data = average_estimate_mean_rejected, aes(x = mean, y = average_estimate),color = "red") +
  geom_point(data = average_estimate_mean_rejected, aes(x = mean, y = average_estimate), color = "red") +
  labs(x = "True μ",y = "Average estimate mean",title = "Average estimate mean vs. true μ - rejected only") +
  theme_minimal() 
```

It is possible to note that the average estimate means of whose nulls are rejected deviate from the true mean (which is always greater) when the true mean is between 1 and 4. This might be as a result of the low power at low impact size. 
